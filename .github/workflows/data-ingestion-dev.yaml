name: "[DEV] Data Ingestion"

on:
  # Automatic execution every hour to check for scheduled jobs
  schedule:
    - cron: "0 * * * *"  # Every hour at minute 0
  
  # Manual trigger with options for specific jobs or groups
  workflow_dispatch:
    inputs:
      job_id:
        description: "Specific job ID to run (e.g., spotify_recently_played)"
        required: false
        type: string
      job_group:
        description: "Job group to run"
        required: false
        type: choice
        options:
          - ""
          - "spotify_fetch_all"
          - "garmin_fetch_all"
          - "chess_fetch_all"
          - "email_test"
          - "ingestion_only"
      force_run:
        description: "Force run all jobs regardless of schedule"
        required: false
        type: boolean
        default: false

env:
  # Spotify credentials
  SPOTIFY_CLIENT_ID: ${{ secrets.SPOTIFY_CLIENT_ID }}
  SPOTIFY_CLIENT_SECRET: ${{ secrets.SPOTIFY_CLIENT_SECRET }}
  SPOTIFY_REDIRECT_URI: ${{ secrets.SPOTIFY_REDIRECT_URI }}
  SPOTIFY_REFRESH_TOKEN: ${{ secrets.SPOTIFY_REFRESH_TOKEN }}
  
  # Strava credentials
  STRAVA_CLIENT_ID: ${{ secrets.STRAVA_CLIENT_ID }}
  STRAVA_CLIENT_SECRET: ${{ secrets.STRAVA_CLIENT_SECRET }}
  STRAVA_REFRESH_TOKEN: ${{ secrets.STRAVA_REFRESH_TOKEN }}
  
  # Todoist credentials
  TODOIST_API_TOKEN: ${{ secrets.TODOIST_API_TOKEN }}
  
  # Garmin credentials
  GARMIN_USERNAME: ${{ secrets.GARMIN_USERNAME }}
  GARMIN_PASSWORD: ${{ secrets.GARMIN_PASSWORD }}
  
  # Withings credentials
  WITHINGS_CLIENT_ID: ${{ secrets.WITHINGS_CLIENT_ID }}
  WITHINGS_CLIENT_SECRET: ${{ secrets.WITHINGS_CLIENT_SECRET }}
  WITHINGS_ACCESS_TOKEN: ${{ secrets.WITHINGS_ACCESS_TOKEN }}
  WITHINGS_REFRESH_TOKEN: ${{ secrets.WITHINGS_REFRESH_TOKEN }}
  WITHINGS_CREDENTIALS_JSON: ${{ secrets.WITHINGS_CREDENTIALS_JSON }}
  
  # GCP credentials for ingestion jobs
  GCP_SERVICE_ACCOUNT_KEY: ${{ secrets.GCP_SERVICE_ACCOUNT_KEY }}
  GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}

  # Email service credentials
  EMAIL_SMTP_FROM: ${{ secrets.EMAIL_SMTP_FROM }}
  EMAIL_SMTP_PASSWORD: ${{ secrets.EMAIL_SMTP_PASSWORD }}

jobs:
  # Job determination phase - decides which jobs should run
  determine-jobs:
    runs-on: ubuntu-latest
    outputs:
      jobs_to_run_encoded: ${{ steps.job-scheduler.outputs.jobs_to_run_encoded }}
      execution_summary: ${{ steps.job-scheduler.outputs.execution_summary }}
      has_jobs: ${{ steps.job-scheduler.outputs.has_jobs }}
      job_ids: ${{ steps.job-scheduler.outputs.job_ids }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python and dependencies
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
      
      - name: Install scheduling dependencies
        run: |
          pip install pyyaml croniter pytz

      - name: Determine jobs to execute
        id: job-scheduler
        run: |
          python << 'EOF'
          import yaml
          import json
          import os
          from datetime import datetime, timezone
          from croniter import croniter
          import pytz

          # Load DEV configuration
          with open('ingestion-config-dev.yaml', 'r') as f:
              config = yaml.safe_load(f)

          # Get inputs
          job_id = "${{ github.event.inputs.job_id }}"
          job_group = "${{ github.event.inputs.job_group }}"
          force_run = "${{ github.event.inputs.force_run }}" == "true"
          target_env = "dev"  # Always DEV for this workflow
          
          # Setup timezone
          tz = pytz.timezone(config['global']['timezone'])
          now = datetime.now(tz)
          
          jobs_to_run = []
          
          def should_job_run(job_config, job_name):
              """Check if a job should run based on its CRON schedule"""
              if not job_config.get('enabled', True):
                  print(f"â¸ï¸  Job {job_name} is disabled")
                  return False
              
              if job_config.get('environment', 'dev') != target_env:
                  print(f"ðŸ”„ Job {job_name} environment mismatch: {job_config.get('environment')} != {target_env}")
                  return False
              
              if force_run:
                  print(f"ðŸš€ Force running job {job_name}")
                  return True
              
              cron_expr = job_config.get('cron')
              if not cron_expr:
                  print(f"âŒ No CRON expression for job {job_name}")
                  return False
              
              try:
                  # Get the previous scheduled time
                  cron = croniter(cron_expr, now)
                  prev_run = cron.get_prev(datetime)
                  
                  # Check if the job should have run within the last hour
                  time_diff = (now - prev_run).total_seconds()
                  should_run = time_diff <= 3600  # 1 hour tolerance
                  
                  if should_run:
                      print(f"âœ… Job {job_name} should run (last scheduled: {prev_run}, diff: {time_diff}s)")
                  else:
                      print(f"â­ï¸  Job {job_name} not scheduled (last scheduled: {prev_run}, diff: {time_diff}s)")
                  
                  return should_run
              except Exception as e:
                  print(f"âŒ Error checking schedule for {job_name}: {e}")
                  return False

          # Determine which jobs to run
          if job_id:
              # Run specific job
              if job_id in config['jobs']:
                  if should_job_run(config['jobs'][job_id], job_id):
                      jobs_to_run.append({
                          'job_id': job_id,
                          'command': config['jobs'][job_id]['command'],
                          'description': config['jobs'][job_id]['description'],
                          'service': config['jobs'][job_id]['service']
                      })
                  else:
                      print(f"âŒ Job {job_id} should not run at this time")
              else:
                  print(f"âŒ Job {job_id} not found in configuration")
          
          elif job_group:
              # Run job group
              if job_group in config['job_groups']:
                  group_jobs = config['job_groups'][job_group]['jobs']
                  print(f"ðŸ“¦ Processing job group '{job_group}' with {len(group_jobs)} jobs")
                  
                  for group_job_id in group_jobs:
                      if group_job_id in config['jobs']:
                          if should_job_run(config['jobs'][group_job_id], group_job_id):
                              jobs_to_run.append({
                                  'job_id': group_job_id,
                                  'command': config['jobs'][group_job_id]['command'],
                                  'description': config['jobs'][group_job_id]['description'],
                                  'service': config['jobs'][group_job_id]['service']
                              })
                      else:
                          print(f"âŒ Job {group_job_id} from group {job_group} not found")
              else:
                  print(f"âŒ Job group {job_group} not found")
          
          else:
              # Auto-scheduled execution - check all jobs
              print(f"ðŸ• Checking all jobs for scheduled execution at {now}")
              
              for job_name, job_config in config['jobs'].items():
                  if should_job_run(job_config, job_name):
                      jobs_to_run.append({
                          'job_id': job_name,
                          'command': job_config['command'],
                          'description': job_config['description'],
                          'service': job_config['service']
                      })

          # Output results
          print(f"\nðŸ“Š Execution Summary (DEV):")
          print(f"   â€¢ Environment: {target_env}")
          print(f"   â€¢ Trigger: {'Manual' if job_id or job_group or force_run else 'Scheduled'}")
          print(f"   â€¢ Jobs to run: {len(jobs_to_run)}")
          
          if jobs_to_run:
              print(f"   â€¢ Job list:")
              for job in jobs_to_run:
                  print(f"     - {job['job_id']} ({job['service']})")
          
          # Set outputs - avoid secret detection by encoding
          import base64
          jobs_json = json.dumps(jobs_to_run)
          jobs_encoded = base64.b64encode(jobs_json.encode()).decode()
          has_jobs = "true" if len(jobs_to_run) > 0 else "false"
          
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f"jobs_to_run_encoded={jobs_encoded}\n")
              f.write(f"execution_summary=Environment: DEV, Jobs: {len(jobs_to_run)}\n")
              f.write(f"has_jobs={has_jobs}\n")
              
              # Also write individual job IDs for matrix (safer approach)
              job_ids = [job['job_id'] for job in jobs_to_run]
              f.write(f"job_ids={json.dumps(job_ids)}\n")
          
          EOF

  # Execution phase - runs the determined jobs in parallel
  execute-jobs:
    needs: determine-jobs
    if: needs.determine-jobs.outputs.has_jobs == 'true'
    runs-on: ubuntu-latest
    
    strategy:
      matrix:
        job_id: ${{ fromJson(needs.determine-jobs.outputs.job_ids || '[]') }}
      max-parallel: 3
      fail-fast: false
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python with uv
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
      
      - name: Install uv
        run: |
          curl -LsSf https://astral.sh/uv/install.sh | sh
          echo "$HOME/.cargo/bin" >> $GITHUB_PATH
      
      - name: Install dependencies with uv
        run: |
          uv pip install --system -e .

      - name: Check GCS credentials and authenticate
        run: |
          if [ -n "$GCP_KEY" ]; then
            echo "âœ… GCP credentials available (length: ${#GCP_KEY})"
            echo "ðŸ” Setting up Google Cloud authentication"
          else
            echo "âŒ GCP_SERVICE_ACCOUNT_KEY secret appears to be empty"
            echo "âš ï¸ Skipping GCS authentication - please check your GitHub secrets configuration"
            echo "SKIP_GCS=true" >> $GITHUB_ENV
            exit 0
          fi
        env:
          GCP_KEY: ${{ secrets.GCP_SERVICE_ACCOUNT_KEY }}

      - name: Authenticate to Google Cloud
        if: env.SKIP_GCS != 'true'
        uses: google-github-actions/auth@v1
        with:
          credentials_json: ${{ secrets.GCP_SERVICE_ACCOUNT_KEY }}

      - name: Set up Google Cloud SDK
        if: env.SKIP_GCS != 'true'
        uses: google-github-actions/setup-gcloud@v1
        with:
          project_id: ${{ secrets.GCP_PROJECT_ID }}
          install_components: gsutil

      - name: Decode job information and validate dependencies
        env:
          JOB_ID: ${{ matrix.job_id }}
          JOBS_ENCODED: ${{ needs.determine-jobs.outputs.jobs_to_run_encoded }}
        run: |
          python << 'EOF'
          import os
          import yaml
          import json
          import base64
          
          job_id = os.environ['JOB_ID']
          
          # Decode job information
          jobs_encoded = os.environ['JOBS_ENCODED']
          jobs_json = base64.b64decode(jobs_encoded).decode()
          all_jobs = json.loads(jobs_json)
          
          # Find current job info
          current_job = None
          for job in all_jobs:
              if job['job_id'] == job_id:
                  current_job = job
                  break
          
          if not current_job:
              print(f"âŒ Job {job_id} not found in decoded jobs")
              exit(1)
          
          # Store job info for next steps
          with open('current_job.json', 'w') as f:
              json.dump(current_job, f)
          
          print(f"âœ… Job info decoded: {current_job['job_id']} ({current_job['service']})")
          print(f"ðŸ“ Description: {current_job['description']}")
          print(f"ðŸ”§ Command: {current_job['command']}")
          
          job_id = current_job['job_id']
          
          # Load config to get dependencies
          with open('ingestion-config-dev.yaml', 'r') as f:
              config = yaml.safe_load(f)
          
          if job_id in config['jobs']:
              dependencies = config['jobs'][job_id].get('dependencies', [])
              missing_deps = []
              
              for dep in dependencies:
                  if not os.getenv(dep):
                      missing_deps.append(dep)
              
              if missing_deps:
                  print(f"âŒ Missing dependencies for {job_id}: {missing_deps}")
                  exit(1)
              else:
                  print(f"âœ… All dependencies available for {job_id}")
          else:
              print(f"âœ… Job {job_id} not found in config (skipping dependency check)")
          EOF

      - name: Execute job
        id: execute
        run: |
          # Load job info from previous step
          JOB_INFO=$(cat current_job.json)
          JOB_ID=$(echo "$JOB_INFO" | python3 -c "import json,sys; print(json.load(sys.stdin)['job_id'])")
          DESCRIPTION=$(echo "$JOB_INFO" | python3 -c "import json,sys; print(json.load(sys.stdin)['description'])")
          COMMAND=$(echo "$JOB_INFO" | python3 -c "import json,sys; print(json.load(sys.stdin)['command'])")
          
          echo "ðŸš€ Executing DEV job: $JOB_ID"
          echo "ðŸ“ Description: $DESCRIPTION"
          echo "ðŸ”§ Command: $COMMAND"
          echo "â° Started at: $(date -u '+%Y-%m-%d %H:%M:%S UTC')"
          
          # Execute the command with timeout (shorter for DEV)
          timeout 15m $COMMAND
          
          echo "âœ… DEV job completed successfully at: $(date -u '+%Y-%m-%d %H:%M:%S UTC')"

      - name: Upload generated files to GCS
        if: success() && env.SKIP_GCS != 'true'
        run: |
          # Find and upload any generated .jsonl files
          JSONL_FILES=$(find . -name "*.jsonl" -type f -newer /proc/self 2>/dev/null | head -10)
          
          if [ -n "$JSONL_FILES" ]; then
            echo "ðŸ“ Found JSONL files to upload:"
            echo "$JSONL_FILES"
            
            # Get service name from job info
            SERVICE=$(cat current_job.json | python3 -c "import json,sys; print(json.load(sys.stdin)['service'])")
            JOB_ID=$(cat current_job.json | python3 -c "import json,sys; print(json.load(sys.stdin)['job_id'])")
            
            for file in $JSONL_FILES; do
              echo "â¬†ï¸  Uploading: $file"
              gsutil cp "$file" "gs://ela-dp-dev/$SERVICE/landing/" \
                || echo "âŒ Upload failed for $file"
            done
            
            echo "âœ… Upload completed for DEV job: $JOB_ID"
          else
            JOB_ID=$(cat current_job.json | python3 -c "import json,sys; print(json.load(sys.stdin)['job_id'])")
            echo "â„¹ï¸  No JSONL files found to upload for DEV job: $JOB_ID"
          fi

      - name: Show generated files (GCS upload skipped)
        if: success() && env.SKIP_GCS == 'true'
        run: |
          echo "ðŸ“ Generated files (GCS upload was skipped):"
          find . -name "*.jsonl" -type f 2>/dev/null | head -10 || echo "No JSONL files found"
          echo ""
          echo "ðŸ’¡ To enable GCS upload:"
          echo "   1. Verify GCP_SERVICE_ACCOUNT_KEY secret contains valid JSON"
          echo "   2. Verify GCP_PROJECT_ID secret is set"
          echo "   3. Ensure service account has Storage Admin permissions"

      - name: Capture job logs on failure
        if: failure()
        run: |
          # Get job info from file
          if [ -f current_job.json ]; then
            JOB_ID=$(cat current_job.json | python3 -c "import json,sys; print(json.load(sys.stdin)['job_id'])")
            SERVICE=$(cat current_job.json | python3 -c "import json,sys; print(json.load(sys.stdin)['service'])")
            COMMAND=$(cat current_job.json | python3 -c "import json,sys; print(json.load(sys.stdin)['command'])")
            DESCRIPTION=$(cat current_job.json | python3 -c "import json,sys; print(json.load(sys.stdin)['description'])")
          else
            JOB_ID="${{ matrix.job_id }}"
            SERVICE="unknown"
            COMMAND="unknown"
            DESCRIPTION="unknown"
          fi
          
          echo "âŒ DEV Job $JOB_ID failed"
          echo "ðŸ“‹ Capturing logs and environment info"
          
          # Create failure report
          cat > job_failure_report.txt << EOF
          Job Failure Report (DEV)
          =========================
          Job ID: $JOB_ID
          Service: $SERVICE
          Command: $COMMAND
          Description: $DESCRIPTION
          Timestamp: $(date -u '+%Y-%m-%d %H:%M:%S UTC')
          Environment: DEV
          Workflow Run: ${{ github.run_id }}
          
          Recent Logs:
          $(tail -50 /var/log/syslog 2>/dev/null || echo 'System logs not available')
          EOF

      - name: Upload failure logs
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: dev-job-failure-logs-${{ matrix.job_id }}
          path: job_failure_report.txt
          retention-days: 7

  # Summary phase - generates execution report
  execution-summary:
    needs: [determine-jobs, execute-jobs]
    if: always()
    runs-on: ubuntu-latest
    
    steps:
      - name: Generate execution summary
        run: |
          echo "# ðŸ“Š Data Ingestion DEV Environment Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Execution Details:**" >> $GITHUB_STEP_SUMMARY
          echo "- **Timestamp:** $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY
          echo "- **Environment:** ðŸ”§ **DEV**" >> $GITHUB_STEP_SUMMARY
          echo "- **Trigger:** ${{ github.event_name == 'workflow_dispatch' && 'Manual' || 'Scheduled' }})" >> $GITHUB_STEP_SUMMARY
          echo "- **${{ needs.determine-jobs.outputs.execution_summary }}**" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Job results summary
          if [ "${{ needs.execute-jobs.result }}" = "success" ]; then
            echo "âœ… **Status:** All DEV jobs completed successfully" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ needs.execute-jobs.result }}" = "failure" ]; then
            echo "âŒ **Status:** Some DEV jobs failed (check individual job logs)" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ needs.execute-jobs.result }}" = "skipped" ]; then
            echo "â­ï¸ **Status:** No DEV jobs were scheduled to run" >> $GITHUB_STEP_SUMMARY
          else
            echo "âš ï¸ **Status:** DEV jobs execution was cancelled or had other issues" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Jobs Matrix:**" >> $GITHUB_STEP_SUMMARY
          
          # Parse and display job results
          jobs_encoded='${{ needs.determine-jobs.outputs.jobs_to_run_encoded }}'
          if [ "$jobs_encoded" != "" ]; then
            python3 -c "
          import json, base64
          jobs_encoded = '${{ needs.determine-jobs.outputs.jobs_to_run_encoded }}'
          if jobs_encoded:
              jobs_json = base64.b64decode(jobs_encoded).decode()
              jobs = json.loads(jobs_json)
              for job in jobs:
                  status = 'âœ…' if '${{ needs.execute-jobs.result }}' == 'success' else 'âŒ' if '${{ needs.execute-jobs.result }}' == 'failure' else 'â­ï¸'
                  print(f'- **{job[\"job_id\"]}** ({job[\"service\"]}) {status}')
                  print(f'  - _{job[\"description\"]}_')
          else:
              print('- No jobs were executed in this DEV run')
          " >> $GITHUB_STEP_SUMMARY
          else
            echo "- No jobs were executed in this DEV run" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "---" >> $GITHUB_STEP_SUMMARY
          echo "_DEV Workflow run: [${{ github.run_id }}](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})_" >> $GITHUB_STEP_SUMMARY