# Plan d'ImplÃ©mentation : AI Coach pour Running Training

## Vision & Objectifs

CrÃ©er un **AI Coach** qui gÃ©nÃ¨re des plans d'entraÃ®nement hebdomadaires personnalisÃ©s en exploitant :
- Les donnÃ©es de santÃ© Garmin (sommeil, HRV, stress, body battery, FC repos)
- Les activitÃ©s sportives (GPS, zones cardio, charge d'entraÃ®nement)
- Un document de contexte utilisateur (objectifs, contraintes, prÃ©fÃ©rences)
- Une approche hybride : plan de cycle (4-12 semaines) + rÃ©gÃ©nÃ©ration hebdomadaire adaptative

**LLM** : Claude Opus 4.5 (API Anthropic)
**Interface** : API REST pure intÃ©grÃ©e dans FastAPI existante
**Stockage contexte** : Google Cloud Storage + API upload

## Use Cases MVP (Phase Initiale)

### Setup Initial (Manuel, 1x par cycle)
1. **GÃ©nÃ©ration du profil coureur** : Analyser 90 jours d'historique pour crÃ©er un profil dÃ©taillÃ© (niveau, forces/faiblesses, zones, tendances)
2. **GÃ©nÃ©ration de cycle complet** : CrÃ©er un plan d'entraÃ®nement de 4-12 semaines avec vue hebdomadaire
3. **Configuration cycle** : Ã‰diter `cycle_config.yaml` pour activer le nouveau cycle

### Automatisation Hebdomadaire (CRON Dimanche 21h)
4. **Weekly Review** : Analyse complÃ¨te de la semaine Ã©coulÃ©e (prÃ©vu vs rÃ©alisÃ©, santÃ©, sommeil) â†’ `.md` gÃ©nÃ©rÃ©
5. **Weekly Plan AdaptÃ©** : GÃ©nÃ©ration du plan semaine suivante basÃ© sur review + cycle + philosophie d'entraÃ®nement â†’ `.md` gÃ©nÃ©rÃ©

**Approche** :
- **Single-user hardcodÃ©** : `user_id = "user_etienne"` partout, aucune auth pour MVP
- **Orchestration automatique** : Cloud Scheduler CRON trigger l'orchestrateur chaque dimanche
- **Cycle config YAML** : Fichier source of truth pour gÃ©rer les cycles actifs et leur organisation
- **Stockage hybride** : BigQuery (analytics) + GCS (fichiers .md lisibles)
- **Philosophie d'entraÃ®nement** : IntÃ©grÃ©e dans le contexte (ex: 80% Zone 2, max 2 hard sessions/semaine)
- **Tests complets** : Couverture unitaire + intÃ©gration + mocks Claude

## Architecture Globale

### IntÃ©gration dans l'Architecture Existante

```
Flux Actuel :
Garmin API â†’ Fetcher â†’ GCS Landing â†’ Ingestor â†’ BigQuery (normalized/lake/hub/product) â†’ FastAPI

Nouveau Flux AI Coach (AutomatisÃ© Hebdo) :
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Cloud Scheduler (CRON - Dimanche 21h)                  â”‚
â”‚  â””â”€> POST /api/ai-coach/orchestrate-weekly              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ORCHESTRATEUR (/src/services/ai_coach/orchestrator.py) â”‚
â”‚                                                          â”‚
â”‚  1. Lire cycle_config.yaml (GCS) â†’ cycle actif ?        â”‚
â”‚  2. RÃ©cupÃ¨re plan semaine Ã©coulÃ©e (BigQuery)            â”‚
â”‚  3. RÃ©cupÃ¨re activitÃ©s rÃ©elles (BigQuery)               â”‚
â”‚  4. RÃ©cupÃ¨re santÃ©/sommeil 7j (BigQuery)                â”‚
â”‚  5. Claude â†’ Weekly Review (.md complet)                â”‚
â”‚  6. Claude â†’ Weekly Plan adaptÃ©                         â”‚
â”‚  7. Store review + plan (BigQuery + GCS .md)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Nouveaux Composants

**1. Service Layer** : `/src/services/ai_coach/` (10 modules)
- `config.py` : Configuration (constante `USER_ID = "user_etienne"`)
- `anthropic_client.py` : Wrapper Claude API avec tracking tokens
- `data_aggregator.py` : AgrÃ©gation intelligente donnÃ©es BigQuery
- `prompt_builder.py` : Construction prompts structurÃ©s
- `profile_generator.py` : GÃ©nÃ©ration profil coureur
- `plan_generator.py` : GÃ©nÃ©ration cycle complet + plans hebdo
- `weekly_reviewer.py` â­ : GÃ©nÃ©ration weekly review (.md)
- `orchestrator.py` â­ : Orchestration hebdo complÃ¨te (review + plan)
- `response_parser.py` : Parsing et validation rÃ©ponses Claude
- `gcs_manager.py` : Upload/retrieval GCS (contextes, config YAML, .md)

**2. API Layer** : `/api/routers/ai_coach.py`
- 7 endpoints REST simplifiÃ©s (vs 12 initialement)
- Pydantic models : `/api/models/ai_coach.py`

**3. Data Layer** : BigQuery `dp_product_dev/prd`
- 6 nouvelles tables (profils, cycles, plans, reviews, contexts, executions)

**4. Storage** : Google Cloud Storage
- Bucket : `ela-dataplatform-ai-coach-contexts`
- **Cycle Config YAML** â­ : Source of truth pour cycles actifs
- **Organisation par cycle** : Chaque cycle a son dossier (reviews/, plans/)
- **Stockage hybride** : .md dans GCS + contenu dans BigQuery

## Cycle Config YAML (Source of Truth)

### Fichier : `gs://ela-dataplatform-ai-coach-contexts/user_etienne/cycle_config.yaml`

```yaml
# Configuration du cycle actif - Ã©ditÃ© manuellement lors du setup d'un nouveau cycle
active_cycle:
  cycle_id: "ecotrail-paris-2026"
  name: "PrÃ©paration Ecotrail Paris 80km"
  start_date: "2026-01-06"
  end_date: "2026-03-23"
  race_date: "2026-03-21"

  # RÃ©fÃ©rences aux ressources (chemins relatifs dans le bucket)
  context_file: "contexts/ecotrail_2026.json"
  profile_file: "profiles/profile_2026-01-05.json"
  cycle_plan_file: "cycles/ecotrail-2026/cycle_plan.json"

  # Organisation des outputs (gÃ©nÃ©rÃ© automatiquement chaque semaine)
  outputs:
    reviews_folder: "cycles/ecotrail-2026/reviews"
    plans_folder: "cycles/ecotrail-2026/plans"

# Historique des cycles passÃ©s (pour rÃ©fÃ©rence)
past_cycles:
  - cycle_id: "marathon-automne-2025"
    name: "Marathon Paris Sub 3h45"
    start_date: "2025-09-01"
    end_date: "2025-11-15"
    race_date: "2025-11-10"
    status: "completed"
    final_result: "3:42:18"
```

**Usage** :
- L'orchestrateur lit ce fichier Ã  chaque exÃ©cution hebdo
- Si `today` n'est pas dans `[start_date, end_date]` â†’ sortie silencieuse (pas de cycle actif)
- Sinon â†’ charge les ressources du cycle et gÃ©nÃ¨re review + plan dans les bons folders

## Structure Google Cloud Storage

```
gs://ela-dataplatform-ai-coach-contexts/
â””â”€â”€ user_etienne/
    â”œâ”€â”€ cycle_config.yaml â­ (source of truth - Ã©ditÃ© manuellement)
    â”‚
    â”œâ”€â”€ contexts/
    â”‚   â”œâ”€â”€ ecotrail_2026.json
    â”‚   â””â”€â”€ marathon_paris_2026.json
    â”‚
    â”œâ”€â”€ profiles/
    â”‚   â”œâ”€â”€ profile_2026-01-05.json
    â”‚   â””â”€â”€ profile_2026-07-30.json
    â”‚
    â””â”€â”€ cycles/
        â”œâ”€â”€ ecotrail-2026/
        â”‚   â”œâ”€â”€ cycle_plan.json (gÃ©nÃ©rÃ© par generate-cycle)
        â”‚   â”œâ”€â”€ reviews/
        â”‚   â”‚   â”œâ”€â”€ week_01_review.md â­ (gÃ©nÃ©rÃ© chaque dimanche)
        â”‚   â”‚   â”œâ”€â”€ week_02_review.md
        â”‚   â”‚   â””â”€â”€ ...
        â”‚   â””â”€â”€ plans/
        â”‚       â”œâ”€â”€ week_02_plan.md â­ (plan pour semaine suivante)
        â”‚       â”œâ”€â”€ week_03_plan.md
        â”‚       â””â”€â”€ ...
        â”‚
        â””â”€â”€ marathon-2026/
            â”œâ”€â”€ cycle_plan.json
            â”œâ”€â”€ reviews/
            â””â”€â”€ plans/
```

## SchÃ©mas BigQuery (6 Tables)

### Table 1 : `ai_coach__runner_profiles`
Stocke les profils coureurs gÃ©nÃ©rÃ©s par l'IA.

```sql
CREATE TABLE dp_product_dev.ai_coach__runner_profiles (
  profile_id STRING NOT NULL,
  user_id STRING NOT NULL,
  created_at TIMESTAMP NOT NULL,

  -- MÃ©tadonnÃ©es gÃ©nÃ©ration
  generated_by STRING,                    -- "claude-opus-4.5-20251101"
  generation_prompt_tokens INT64,
  generation_completion_tokens INT64,

  -- PÃ©riode d'analyse
  analysis_start_date DATE,
  analysis_end_date DATE,
  total_activities_analyzed INT64,

  -- Profil complet (JSON)
  profile_json STRING,

  -- Insights clÃ©s (extraction pour requÃªtes rapides)
  runner_level STRING,                    -- "beginner"|"intermediate"|"advanced"|"elite"
  weekly_volume_km FLOAT64,
  vo2_max_estimate FLOAT64,
  primary_strengths ARRAY<STRING>,
  primary_weaknesses ARRAY<STRING>,
  recommended_training_zones STRUCT<
    zone1_hr_range STRING,
    zone2_hr_range STRING,
    zone3_hr_range STRING,
    zone4_hr_range STRING,
    zone5_hr_range STRING
  >,

  is_active BOOLEAN,                      -- Un seul profil actif par user
  _dp_updated_at TIMESTAMP
)
CLUSTER BY user_id, is_active;
```

### Table 2 : `ai_coach__training_cycles`
Stocke les plans de cycle complets (4-12 semaines).

```sql
CREATE TABLE dp_product_dev.ai_coach__training_cycles (
  cycle_id STRING NOT NULL,
  user_id STRING NOT NULL,
  profile_id STRING NOT NULL,
  context_gcs_path STRING NOT NULL,       -- gs://bucket/user/contexts/uuid.json
  created_at TIMESTAMP NOT NULL,

  -- DÃ©finition du cycle
  cycle_start_date DATE NOT NULL,
  cycle_end_date DATE NOT NULL,
  cycle_goal STRING,                      -- "marathon_sub_3h30", "build_base"
  total_weeks INT64,

  -- Plan complet (JSON)
  cycle_plan_json STRING,

  -- RÃ©sumÃ©s hebdomadaires
  weekly_summaries ARRAY<STRUCT<
    week_number INT64,
    week_start_date DATE,
    total_km FLOAT64,
    key_workouts ARRAY<STRING>,
    focus STRING
  >>,

  -- MÃ©tadonnÃ©es
  generated_by STRING,
  generation_prompt_tokens INT64,
  generation_completion_tokens INT64,

  status STRING,                          -- "active"|"completed"|"abandoned"
  _dp_updated_at TIMESTAMP
)
CLUSTER BY user_id, status;
```

### Table 3 : `ai_coach__weekly_plans`
Stocke les plans hebdomadaires (7 jours) rÃ©gÃ©nÃ©rÃ©s chaque semaine.

```sql
CREATE TABLE dp_product_dev.ai_coach__weekly_plans (
  plan_id STRING NOT NULL,
  cycle_id STRING,                        -- FK (nullable pour plans standalone)
  user_id STRING NOT NULL,
  profile_id STRING NOT NULL,
  context_gcs_path STRING NOT NULL,
  created_at TIMESTAMP NOT NULL,

  -- Semaine
  week_start_date DATE NOT NULL,
  week_end_date DATE NOT NULL,
  week_number_in_cycle INT64,

  -- MÃ©tadonnÃ©es
  generated_by STRING,
  generation_prompt_tokens INT64,
  generation_completion_tokens INT64,
  regeneration_reason STRING,             -- "initial"|"weekly_update"|"user_feedback"

  -- Snapshots d'input (pour traÃ§abilitÃ©)
  input_health_snapshot STRING,           -- JSON des 7 derniers jours de santÃ©
  input_activity_snapshot STRING,         -- JSON des 7 derniÃ¨res activitÃ©s

  -- Plan hebdomadaire (JSON complet)
  weekly_plan_json STRING,

  -- Plan Markdown â­ NOUVEAU
  plan_markdown STRING,                   -- Le .md du plan gÃ©nÃ©rÃ© par Claude
  gcs_markdown_path STRING,               -- gs://bucket/.../plans/week_XX_plan.md

  -- SÃ©ances structurÃ©es (pour accÃ¨s API facile)
  daily_workouts ARRAY<STRUCT<
    date DATE,
    day_name STRING,
    workout_type STRING,                  -- "easy_run"|"intervals"|"long_run"|"rest"
    planned_distance_km FLOAT64,
    planned_duration_min INT64,
    planned_pace_range STRING,
    planned_hr_zone STRING,
    workout_description STRING,
    rationale STRING
  >>,

  status STRING,                          -- "active"|"completed"|"superseded"
  _dp_updated_at TIMESTAMP
)
PARTITION BY week_start_date
CLUSTER BY user_id, status;
```

### Table 4 : `ai_coach__weekly_reviews` â­ NOUVEAU
Stocke les compte-rendus hebdomadaires gÃ©nÃ©rÃ©s automatiquement.

```sql
CREATE TABLE dp_product_dev.ai_coach__weekly_reviews (
  review_id STRING NOT NULL,
  user_id STRING NOT NULL,
  cycle_id STRING NOT NULL,
  week_start_date DATE NOT NULL,
  week_end_date DATE NOT NULL,
  week_number_in_cycle INT64,
  created_at TIMESTAMP NOT NULL,

  -- Plan comparÃ©
  plan_id STRING,                        -- FK vers weekly_plans

  -- Analyse activitÃ©s (rÃ©sumÃ©)
  total_planned_km FLOAT64,
  total_actual_km FLOAT64,
  compliance_pct FLOAT64,
  sessions_completed INT64,
  sessions_planned INT64,

  -- MÃ©triques santÃ© moyennes semaine
  avg_sleep_score FLOAT64,
  avg_sleep_duration_hours FLOAT64,
  avg_hrv FLOAT64,
  avg_body_battery_recovery FLOAT64,
  avg_resting_hr INT64,

  -- Review complÃ¨te (Markdown) â­
  review_markdown STRING,                -- Le .md complet gÃ©nÃ©rÃ© par Claude
  gcs_markdown_path STRING,              -- gs://bucket/.../reviews/week_XX_review.md

  -- MÃ©tadonnÃ©es AI
  generated_by STRING,
  generation_prompt_tokens INT64,
  generation_completion_tokens INT64,

  _dp_updated_at TIMESTAMP
)
PARTITION BY week_start_date
CLUSTER BY user_id, cycle_id;
```

### Table 5 : `ai_coach__plan_execution`
Tracking prÃ©vu vs rÃ©alisÃ© (post-MVP mais utile pour architecture).

```sql
CREATE TABLE dp_product_dev.ai_coach__plan_execution (
  execution_id STRING NOT NULL,
  plan_id STRING NOT NULL,
  user_id STRING NOT NULL,
  date DATE NOT NULL,

  -- PrÃ©vu (dÃ©normalisÃ©)
  planned_workout_type STRING,
  planned_distance_km FLOAT64,
  planned_duration_min INT64,

  -- RÃ©alisÃ© (lien vers activitÃ©s Garmin)
  activity_id INT64,                      -- FK vers hub_health__svc_activities
  actual_distance_km FLOAT64,
  actual_duration_min INT64,
  actual_avg_hr INT64,

  -- Compliance
  compliance_status STRING,               -- "completed"|"partial"|"missed"|"rest_day"
  distance_compliance_pct FLOAT64,

  user_notes STRING,
  _dp_updated_at TIMESTAMP
)
PARTITION BY date
CLUSTER BY user_id, plan_id;
```

### Table 6 : `ai_coach__context_documents`
MÃ©tadonnÃ©es des contextes utilisateur stockÃ©s dans GCS.

```sql
CREATE TABLE dp_product_dev.ai_coach__context_documents (
  context_id STRING NOT NULL,
  user_id STRING NOT NULL,
  uploaded_at TIMESTAMP NOT NULL,

  gcs_path STRING NOT NULL,               -- gs://bucket/user/contexts/uuid.json

  -- MÃ©tadonnÃ©es du contexte
  context_type STRING,                    -- "race_goal"|"general_training"
  race_date DATE,
  race_distance STRING,                   -- "marathon"|"half_marathon"|"10k"

  -- RÃ©sumÃ© (extrait du doc)
  objective STRING,
  constraints ARRAY<STRING>,
  preferences ARRAY<STRING>,

  -- Philosophie d'entraÃ®nement â­ NOUVEAU
  training_philosophy_json STRING,        -- JSON avec zone distribution, rules, etc.

  -- Usage tracking
  used_in_cycles ARRAY<STRING>,
  used_in_plans ARRAY<STRING>,

  is_active BOOLEAN,
  _dp_updated_at TIMESTAMP
)
CLUSTER BY user_id, is_active;
```

## Structure Google Cloud Storage

**Bucket** : `ela-dataplatform-ai-coach-contexts`

```
gs://ela-dataplatform-ai-coach-contexts/
â”œâ”€â”€ user_etienne/
â”‚   â”œâ”€â”€ contexts/
â”‚   â”‚   â”œâ”€â”€ 2026-01-15_marathon_sub_3h30.json
â”‚   â”‚   â””â”€â”€ 2026-02-01_base_building.json
â”‚   â”œâ”€â”€ profiles/                   # Optionnel : historique profils
â”‚   â””â”€â”€ generations/                # Optionnel : raw AI responses
â””â”€â”€ templates/                      # Templates d'exemple
    â”œâ”€â”€ marathon_template.json
    â””â”€â”€ base_building_template.json
```

**Format Context Document** (JSON uploadÃ© par user) :
```json
{
  "context_id": "uuid-v4",
  "user_id": "user_etienne",
  "created_at": "2026-01-15T10:00:00Z",
  "objective": {
    "type": "race",
    "race_type": "marathon",
    "race_date": "2026-04-20",
    "target_time": "3:30:00",
    "current_level": "intermediate"
  },
  "constraints": {
    "weekly_sessions": 4,
    "max_weekly_volume_km": 80,
    "unavailable_days": ["Sunday morning before 9am"],
    "injury_history": ["IT band syndrome - recovered 2024"],
    "equipment": ["Garmin Forerunner 965", "HRM-Pro strap"]
  },
  "preferences": {
    "training_style": "structured with some flexibility",
    "terrain": "mix of road (70%) and trail (30%)",
    "preferred_workout_types": ["tempo runs", "long runs", "interval sessions"],
    "avoid": ["track workouts", "very early morning runs"],
    "long_run_day": "Saturday",
    "hard_session_day": "Wednesday"
  },
  "training_philosophy": {
    "volume_distribution": {
      "zone_2_pct": 80,
      "zone_3_pct": 10,
      "zone_4_5_pct": 10
    },
    "weekly_structure": {
      "hard_sessions_max": 2,
      "recovery_days_min": 1,
      "long_run_pct_of_weekly_volume": 30
    },
    "progression_rules": {
      "weekly_volume_increase_max_pct": 10,
      "long_run_increase_max_km": 3,
      "consecutive_hard_days_max": 2
    },
    "adaptation_priorities": [
      "Sleep quality first - skip hard session if sleep <7h for 3 days",
      "HRV baseline -10% = recovery week",
      "Body Battery <25 at bedtime = reduce intensity next day"
    ]
  },
  "notes": "Prefer progressive long runs. Need recovery runs to be truly easy (Zone 2 max). Can handle 1-2 hard sessions per week but need full recovery days between."
}
```

## API Endpoints (FastAPI)

**Router** : `/api/routers/ai_coach.py`

### Endpoints MVP SimplifiÃ©s (7 endpoints vs 12 initialement)

**Setup / Occasionnel** (Manuel, 1x par cycle) :

1. **POST /api/ai-coach/upload-context**
   - Upload contexte utilisateur vers GCS + store metadata
   - Request : `{"context_type": "race_goal", "context_data": {...}}`
   - Response : `{"context_id": "uuid", "gcs_path": "gs://...", "uploaded_at": "..."}`

2. **POST /api/ai-coach/generate-profile**
   - GÃ©nÃ¨re profil coureur (90 jours d'historique, user hardcodÃ©)
   - Request : `{"analysis_days": 90}` (optionnel, default=90)
   - Response : Profil complet JSON + metadata gÃ©nÃ©ration
   - Stocke dans `ai_coach__runner_profiles` avec `is_active=True`

3. **POST /api/ai-coach/generate-cycle**
   - GÃ©nÃ¨re cycle complet 4-12 semaines
   - Request : `{"context_id": "uuid", "profile_id": "uuid", "cycle_start_date": "2026-01-20", "total_weeks": 12}`
   - Response : Cycle complet avec weekly_summaries + cycle_plan_json
   - Stocke dans `ai_coach__training_cycles` + GCS

4. **PUT /api/ai-coach/cycle-config** (Optionnel - facilite Ã©dition YAML via API)
   - Met Ã  jour `cycle_config.yaml` dans GCS
   - Request : `{"cycle_id": "...", "start_date": "...", "end_date": "...", "context_id": "...", "profile_id": "..."}`
   - Response : Confirmation + chemin YAML

**AutomatisÃ© / Hebdomadaire** (AppelÃ© par Cloud Scheduler) :

5. **POST /api/ai-coach/orchestrate-weekly** â­ CÅ’UR DU SYSTÃˆME
   - Orchestration complÃ¨te : review + plan adaptÃ©
   - Request : Aucun (lit cycle_config.yaml)
   - Response : `{"review_id": "...", "plan_id": "...", "review_gcs_path": "...", "plan_gcs_path": "..."}`
   - Flow complet : Lit config â†’ VÃ©rifie cycle actif â†’ GÃ©nÃ¨re review â†’ GÃ©nÃ¨re plan â†’ Store BQ + GCS

**Consultation** (Lecture des rÃ©sultats) :

6. **GET /api/ai-coach/weekly-review/latest**
   - RÃ©cupÃ¨re dernier compte-rendu hebdo (markdown + mÃ©triques)
   - Response : Review complet avec lien GCS vers .md

7. **GET /api/ai-coach/plans/current**
   - RÃ©cupÃ¨re plan actif pour semaine en cours (markdown + daily_workouts)
   - Response : Plan complet avec lien GCS vers .md

**Pydantic Models** : `/api/models/ai_coach.py` (pattern de [activities.py](api/models/activities.py))
- `ContextUploadRequest`, `ContextResponse`, `TrainingPhilosophy`
- `RunnerProfile`, `ProfileGenerationRequest`
- `TrainingCycle`, `CycleGenerationRequest`, `WeeklySummary`
- `WeeklyPlan`, `PlanGenerationRequest`, `DailyWorkout`
- `WeeklyReview`, `ReviewMetrics` â­
- `CycleConfigUpdate` â­
- Utiliser `datetime` de Python standard, pas `Optional` sauf si vraiment nullable

## StratÃ©gie de DonnÃ©es

### AgrÃ©gation des DonnÃ©es pour l'IA

**Principe** : Utiliser les tables **existantes** `hub_health__svc_*` et `product pct_*` via **requÃªtes runtime** (pas de nouveaux modÃ¨les dbt pour MVP).

**Tables Ã  Exploiter** :
- `hub_health__svc_activities` : ActivitÃ©s avec GPS, zones HR, charge
- `hub_health__svc_sleep` : Sommeil, HRV, body battery, FC repos
- Lake models : Training readiness, VO2 max, stress

**Optimisation Tokens** (critique pour coÃ»ts) :
- **Derniers 7 jours** : DonnÃ©es complÃ¨tes dÃ©taillÃ©es (~2K tokens)
- **Jours 8-30** : RÃ©sumÃ© par activitÃ©, pas de GPS (~1K tokens)
- **Jours 31-90** : AgrÃ©gats hebdomadaires uniquement (~500 tokens)
- **Total** : ~8K tokens input (vs 100K sans optimisation)

**Fonctions d'AgrÃ©gation** (`data_aggregator.py`) :
```python
def get_activity_summary_for_profile(user_id: str, days: int = 90) -> dict
def get_health_summary_for_profile(user_id: str, days: int = 90) -> dict
def get_recent_health_snapshot(user_id: str, days: int = 7) -> dict
def get_recent_activity_snapshot(user_id: str, days: int = 7) -> dict
```

## IntÃ©gration Anthropic Claude

### Setup SDK

**DÃ©pendance** : `anthropic>=0.18.0` (Ã  ajouter dans `pyproject.toml`)

**Client Wrapper** (`anthropic_client.py`) :
```python
class ClaudeClient:
    def __init__(self):
        self.client = anthropic.Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
        self.model = "claude-opus-4-5-20251101"

    def generate(self, system_prompt: str, user_prompt: str,
                 max_tokens: int = 4096) -> dict:
        # Appel API avec tracking tokens/coÃ»ts
        # Retourne: {"content": "...", "usage": {...}, "model": "..."}
```

### StratÃ©gie de Prompting

**Principes** :
1. **Structured Output** : Imposer JSON schema strict dans system prompt
2. **Token Optimization** : Sampling intelligent des donnÃ©es
3. **Domain Expertise** : Injecter connaissances physiologie running
4. **Context Layering** : Health â†’ Activity â†’ User Context â†’ Request

**Exemple System Prompt (Profil)** :
```
You are an expert running coach with 20+ years of experience analyzing wearable data.

Task: Create a comprehensive runner profile from provided data.

OUTPUT FORMAT (strict JSON):
{
  "runner_level": "beginner|intermediate|advanced|elite",
  "weekly_volume_km": float,
  "vo2_max_estimate": float,
  "primary_strengths": ["endurance", "consistency"],
  "primary_weaknesses": ["speed_work", "recovery"],
  "training_zones": {...},
  "analysis": {...},
  "recommendations": {...}
}

DATA INTERPRETATION:
- Sleep score >80 = good recovery
- HRV 40-60ms = typical trained runner
- Body Battery <25 at bedtime = overtraining risk
- Zone 2: 70-80% weekly volume ideal
```

**CoÃ»ts EstimÃ©s** (Claude Opus 4.5 : $15/M input, $75/M output) :
- GÃ©nÃ©ration profil : ~8K input + 3K output = **$0.40**
- Plan hebdo : ~5K input + 2K output = **$0.25**
- **Mensuel** (1 profil + 4 plans) : **~$1.60/user**

## Plan d'ImplÃ©mentation (Phases)

### Phase 0 : Foundation (PrÃ©-requis)

**Objectif** : Infrastructure & dÃ©pendances

**TÃ¢ches** :
1. Ajouter `anthropic>=0.18.0` dans [pyproject.toml:19](pyproject.toml)
2. CrÃ©er les 5 tables BigQuery (DDL ci-dessus)
3. CrÃ©er bucket GCS `ela-dataplatform-ai-coach-contexts` avec IAM appropriÃ©
4. Ajouter `ANTHROPIC_API_KEY` dans `.env` local et GCP Secret Manager

**Livrable** : Infrastructure prÃªte

---

### Phase 1 : Service Layer - Data Aggregation

**Objectif** : Couche de rÃ©cupÃ©ration des donnÃ©es depuis BigQuery

**Fichiers Ã  CrÃ©er** :
- `/src/services/ai_coach/__init__.py`
- `/src/services/ai_coach/config.py`
- `/src/services/ai_coach/data_aggregator.py`

**ImplÃ©mentation** :
- Fonctions d'agrÃ©gation intelligente (sampling 7/30/90 jours)
- RequÃªtes vers `hub_health__svc_activities`, `hub_health__svc_sleep`
- Tests unitaires avec mock BigQuery

**Pattern** : Suivre style existant dans `/src/connectors/fetcher/`

**Livrable** : AgrÃ©gation donnÃ©es testÃ©e, <10K tokens par requÃªte

---

### Phase 2 : Service Layer - AI Integration (Profil)

**Objectif** : GÃ©nÃ©ration profil coureur (Use Case #1)

**Fichiers Ã  CrÃ©er** :
- `/src/services/ai_coach/anthropic_client.py`
- `/src/services/ai_coach/prompt_builder.py`
- `/src/services/ai_coach/profile_generator.py`
- `/src/services/ai_coach/response_parser.py`

**ImplÃ©mentation** :
1. **anthropic_client.py** : Wrapper Claude API avec tracking tokens
2. **prompt_builder.py** :
   - `build_profile_system_prompt()` : Instructions + JSON schema
   - `build_profile_user_prompt(data)` : Injection donnÃ©es santÃ©/activitÃ©s
3. **profile_generator.py** :
   - `generate_runner_profile(user_id, days=90)` : Orchestration complÃ¨te
   - Flow : Aggregate data â†’ Build prompt â†’ Call Claude â†’ Parse â†’ Store BQ
4. **response_parser.py** : Validation JSON, extraction champs structurÃ©s

**Stockage** : Insert dans `ai_coach__runner_profiles`, set `is_active=True`

**Livrable** : GÃ©nÃ©ration profil fonctionnelle (test local)

---

### Phase 3 : API Layer - Profile Endpoints

**Objectif** : Exposer gÃ©nÃ©ration profil via API REST

**Fichiers Ã  CrÃ©er** :
- `/api/routers/ai_coach.py`
- `/api/models/ai_coach.py`

**ImplÃ©mentation** :
1. CrÃ©er router FastAPI (pattern similaire Ã  [api/routers/homepage.py](api/routers/homepage.py))
2. Pydantic models : `RunnerProfile`, `ProfileGenerationRequest`
3. Endpoints :
   - `POST /api/ai-coach/generate-profile`
   - `GET /api/ai-coach/profile/latest`
4. Enregistrer router dans [api/main.py:44](api/main.py)

**Tests** : `uvicorn api.main:app --reload` + Postman/curl

**Livrable** : API profil accessible, rÃ©ponses validÃ©es

---

### Phase 4 : Service Layer - Context Management

**Objectif** : Upload/stockage contextes utilisateur

**Fichiers Ã  CrÃ©er** :
- `/src/services/ai_coach/gcs_manager.py`

**ImplÃ©mentation** :
- `upload_context(user_id, context_data)` : Upload GCS + insert metadata BQ
- `get_context(context_id)` : Retrieve from GCS
- Pattern : S'inspirer de `/src/connectors/fetcher/gcs_writer.py`

**API Endpoint** :
- `POST /api/ai-coach/upload-context` dans router existant
- `GET /api/ai-coach/contexts/{context_id}`

**Livrable** : Upload/retrieval contextes fonctionnel

---

### Phase 5 : Service Layer - Plan Generation (Cycle + Weekly)

**Objectif** : GÃ©nÃ©ration cycle complet ET plans hebdo (fusionnÃ©)

**Fichiers Ã  CrÃ©er** :
- `/src/services/ai_coach/plan_generator.py`

**ImplÃ©mentation** :
1. **prompt_builder.py** (extension) :
   - `build_cycle_system_prompt()` : Instructions plan complet multi-semaines
   - `build_cycle_user_prompt(profile, context, health_snapshot)`
   - `build_plan_system_prompt()` : Instructions plan 7 jours avec contexte cycle
   - `build_plan_user_prompt(cycle, profile, context, health_snapshot, week_number)`

2. **plan_generator.py** (2 fonctions principales) :
   - `generate_training_cycle(context_id, profile_id, cycle_start_date, total_weeks)`
     - Flow : Get context (GCS) â†’ Get profile (BQ) â†’ Get recent health â†’ Build prompt â†’ Claude â†’ Parse â†’ Store BQ + GCS
   - `generate_weekly_plan(cycle_id, week_start_date, regeneration_reason)`
     - Flow : Get cycle (BQ) â†’ Get context (GCS) â†’ Get profile (BQ) â†’ Get recent health â†’ Build prompt â†’ Claude â†’ Parse â†’ Store BQ + GCS
     - GÃ©nÃ¨re aussi le .md du plan

3. **response_parser.py** (extension) :
   - Parser cycle complet + weekly_summaries
   - Parser plan hebdo + daily_workouts

**Stockage** :
- Cycle : Insert dans `ai_coach__training_cycles` + GCS JSON
- Plan : Insert dans `ai_coach__weekly_plans` + GCS .md

**Livrable** : GÃ©nÃ©ration cycle + plans testÃ©e localement

---

### Phase 6 : Service Layer - Weekly Reviewer + Orchestrator â­

**Objectif** : CÅ“ur de l'automatisation hebdomadaire

**Fichiers Ã  CrÃ©er** :
- `/src/services/ai_coach/weekly_reviewer.py`
- `/src/services/ai_coach/orchestrator.py`

**ImplÃ©mentation** :

1. **weekly_reviewer.py** :
   - `generate_weekly_review(plan_id, week_start_date, week_end_date)`
   - Flow :
     - Get plan semaine Ã©coulÃ©e (BQ)
     - Get activitÃ©s rÃ©elles (BQ `hub_health__svc_activities`)
     - Get santÃ©/sommeil 7j (BQ `hub_health__svc_sleep`, etc.)
     - Build prompt comparaison prÃ©vu vs rÃ©alisÃ©
     - Claude â†’ GÃ©nÃ¨re review .md complet
     - Parse mÃ©triques clÃ©s (compliance, avg HRV, etc.)
     - Store dans `ai_coach__weekly_reviews` + GCS .md

2. **prompt_builder.py** (extension) :
   - `build_review_system_prompt()` : Instructions analyse complÃ¨te semaine
   - `build_review_user_prompt(planned, actual, health)` : DonnÃ©es structurÃ©es

3. **orchestrator.py** (fonction principale) :
   - `orchestrate_weekly()`
   - Flow complet :
     1. Load `cycle_config.yaml` from GCS
     2. Check if today in `[start_date, end_date]` â†’ exit if not
     3. Calculate week_number from cycle start
     4. Call `weekly_reviewer.generate_weekly_review()` (semaine Ã©coulÃ©e)
     5. Call `plan_generator.generate_weekly_plan()` (semaine suivante, adaptÃ© au review)
     6. Return review_id + plan_id + GCS paths

**Livrable** : Orchestration complÃ¨te testÃ©e localement

---

### Phase 7 : API Layer - Endpoints SimplifiÃ©s

**Objectif** : Exposer 7 endpoints (vs 12 initialement)

**ImplÃ©mentation** :
- 7 endpoints dÃ©finis dans section API Endpoints
- Pattern : Suivre [homepage.py](api/routers/homepage.py) (try/except HTTPException, async, get_bq_client())
- Pydantic models : Toutes les classes dÃ©finies section API Endpoints
- Enregistrer router dans [main.py](api/main.py) : `app.include_router(ai_coach.router, prefix="/api/ai-coach", tags=["ai-coach"])`

**Endpoints clÃ©s** :
- Setup : upload-context, generate-profile, generate-cycle, cycle-config
- Auto : orchestrate-weekly â­
- Consultation : weekly-review/latest, plans/current

**Tests End-to-End** :
1. Upload context â†’ verify GCS + BQ
2. Generate profile â†’ verify BQ + is_active
3. Generate cycle â†’ verify BQ + GCS JSON
4. Update cycle_config.yaml (manuel ou API)
5. Call orchestrate-weekly â†’ verify review + plan gÃ©nÃ©rÃ©s
6. GET review/latest et plans/current â†’ verify .md accessibles

**Livrable** : MVP complet (Setup + Orchestration) fonctionnel en local

---

### Phase 8 : Tests & Validation

**Objectif** : Couverture tests complÃ¨te (unitaire + intÃ©gration)

**Fichiers Ã  CrÃ©er** :
- `/tests/services/ai_coach/test_data_aggregator.py`
- `/tests/services/ai_coach/test_anthropic_client.py`
- `/tests/services/ai_coach/test_response_parser.py`
- `/tests/api/test_ai_coach_router.py`

**ImplÃ©mentation** :

1. **Tests Unitaires Service Layer** :
   - `test_data_aggregator.py` : Mock BigQuery, valider requÃªtes + sampling
   - `test_response_parser.py` : Valider parsing JSON Claude (profil, cycle, plan)
   - Mock responses Claude pour tester sans consommer API

2. **Tests d'IntÃ©gration API** :
   - `test_ai_coach_router.py` : FastAPI TestClient
   - Mock service layer pour tester endpoints isolÃ©ment
   - Valider request/response schemas Pydantic

3. **Test E2E avec Mock Claude** :
   - Mocker `anthropic.Client` pour retourner rÃ©ponses prÃ©dÃ©finies
   - Valider flow complet : context â†’ profile â†’ cycle â†’ plan

**Livrable** : Coverage >80% sur service layer, tous endpoints testÃ©s

---

### Phase 9 : Template Context & Documentation

**Objectif** : CrÃ©er template exemple + upload dans GCS

**Fichiers Ã  CrÃ©er** :
- `/templates/marathon_context_template.json` (local, puis upload GCS)
- Optionnel : `/docs/ai_coach_usage.md` (guide utilisateur)

**Template Marathon** :
```json
{
  "context_id": "example-marathon-sub-3h30",
  "user_id": "user_etienne",
  "created_at": "2026-01-15T10:00:00Z",
  "objective": {
    "type": "race",
    "race_type": "marathon",
    "race_date": "2026-04-20",
    "target_time": "3:30:00",
    "current_level": "intermediate",
    "current_weekly_volume_km": 45
  },
  "constraints": {
    "weekly_sessions": 4,
    "max_weekly_volume_km": 80,
    "unavailable_days": ["Sunday morning before 9am"],
    "injury_history": ["IT band syndrome - recovered 2024"],
    "equipment": ["Garmin Forerunner 965", "HRM-Pro strap"]
  },
  "preferences": {
    "training_style": "structured with some flexibility",
    "terrain": "mix of road (70%) and trail (30%)",
    "preferred_workout_types": ["tempo runs", "long runs", "interval sessions"],
    "avoid": ["track workouts", "very early morning runs"],
    "long_run_day": "Saturday",
    "hard_session_day": "Wednesday"
  },
  "notes": "Prefer progressive long runs. Need recovery runs to be truly easy (Zone 2 max). Can handle 1-2 hard sessions per week but need full recovery days between."
}
```

**Actions** :
1. CrÃ©er template local
2. Utiliser endpoint `POST /api/ai-coach/upload-context` pour uploader
3. VÃ©rifier prÃ©sence dans GCS `templates/marathon_template.json`

**Livrable** : Template prÃªt Ã  l'emploi, documentÃ©

---

### Phase 10 : Deployment & Cloud Scheduler CRON â­

**Objectif** : DÃ©ployer en production + automatisation hebdo

**TÃ¢ches** :

1. **Build & Deploy API** :
   - VÃ©rifier [Dockerfile:22](Dockerfile) inclut Anthropic SDK (`anthropic>=0.18.0`)
   - Ajouter `ANTHROPIC_API_KEY` dans GCP Secret Manager
   - Mettre Ã  jour Cloud Run service `ela-api` pour injecter secret
   - DÃ©ployer via CI/CD existant (merge dans `main`)

2. **Configuration Cloud Scheduler** (nouveau CRON job) :
   ```bash
   gcloud scheduler jobs create http ai-coach-weekly-orchestrator \
     --location=us-central1 \
     --schedule="0 21 * * 0" \
     --time-zone="Europe/Paris" \
     --uri="https://ela-api-xxx.run.app/api/ai-coach/orchestrate-weekly" \
     --http-method=POST \
     --oidc-service-account-email="cloud-scheduler@PROJECT_ID.iam.gserviceaccount.com" \
     --oidc-token-audience="https://ela-api-xxx.run.app"
   ```
   - Schedule : `0 21 * * 0` = Dimanche 21h (Europe/Paris)
   - AuthentifiÃ© via OIDC (Cloud Run service account)

3. **Permissions IAM** :
   - Accorder `roles/run.invoker` Ã  service account du scheduler
   - Accorder `roles/storage.objectAdmin` Ã  Cloud Run pour GCS access

4. **Cycle Config Initial** :
   - Upload `cycle_config.yaml` dans GCS (manuellement ou via API)
   - VÃ©rifier structure correcte

5. **Monitoring & Alertes** :
   - Cloud Logging : Filtrer logs `orchestrate-weekly`
   - Billing alerts : <$50/mois Anthropic API
   - Cloud Monitoring : Dashboard pour latency/errors

**Fichiers** : Aucune modification CI/CD nÃ©cessaire ([.github/workflows/cd-deploy-api.yaml](https://github.com/anthropics/claude-code/blob/main/.github/workflows/cd-deploy-api.yaml) dÃ©jÃ  configurÃ©)

**Livrable** : AI Coach live en production avec CRON hebdo actif

---

### Phase 11+ : Post-MVP (Optionnel)

**Features futures** :
- **Tracking prÃ©vu vs rÃ©alisÃ© automatique** : Lier automatiquement activitÃ©s Garmin aux daily_workouts, calculer compliance
- **Adaptation dynamique en cours de semaine** : Ajuster plan si HRV/sommeil dÃ©gradÃ©s (ex: skip hard session si body battery <25)
- **Conversational interface** : Chat avec historique pour poser questions au coach ("Pourquoi cette sÃ©ance ?", "Puis-je faire plus ?")
- **Multi-utilisateurs** : Ajouter JWT auth, user_id dynamique, migration tables
- **Notifications** : Email/push hebdomadaire avec plan gÃ©nÃ©rÃ© automatiquement (Cloud Scheduler + SendGrid)
- **Visualisations** : Dashboard avec progression profil dans le temps, analyse tendances

## DÃ©cisions Techniques Critiques

### 1. Approche Hybride Cycle + Hebdo

**ImplÃ©mentation** :
- GÃ©nÃ©rer cycle complet 8-12 semaines (stockÃ© dans `ai_coach__training_cycles`)
- RÃ©gÃ©nÃ©rer plan hebdo chaque semaine basÃ© sur :
  - Plan cycle (rÃ©fÃ©rence)
  - ExÃ©cution semaine prÃ©cÃ©dente
  - MÃ©triques santÃ© actuelles
  - Feedback utilisateur

**BÃ©nÃ©fice** : Structure long-terme + adaptation rÃ©aliste

### 2. Versioning Profils & Contextes

**StratÃ©gie** : Versioning temporel avec flag `is_active`
- Un seul profil actif par user
- Historique conservÃ© pour analyse d'Ã©volution
- Contextes immuables (nouveau upload = nouveau context_id)

### 3. Gestion Erreurs AI

**Robustesse** :
- Timeout Claude : 30s max
- Retry : 3 tentatives avec backoff exponentiel
- Validation JSON stricte (reject si malformed)
- Fallback : HTTP 503 si Ã©chec AI
- Logging : Tous prompts/responses dans GCS pour debug

### 4. Optimisation CoÃ»ts

**StratÃ©gies** :
- Sampling intelligent (section Data Aggregation)
- Rate limiting : 1 profil/jour, 1 plan/semaine par user
- max_tokens=4096 pour cap output
- Monitoring usage dans BQ
- Billing alerts Ã  $50/mois

**CoÃ»t estimÃ©** : $1.40/mois/user (acceptable MVP)

## Risques & Mitigations

### Risque 1 : Consistance Prompts
- **ProblÃ¨me** : Claude peut gÃ©nÃ©rer formats inconsistants
- **Mitigation** : Tests extensifs, exemples dans prompts, validation stricte

### Risque 2 : Performance BigQuery
- **ProblÃ¨me** : RequÃªtes runtime lentes (>5s)
- **Mitigation** : Utiliser product layer, clustering, optionnel dbt materialized views si besoin

### Risque 3 : SÃ©curitÃ© Plans
- **ProblÃ¨me** : IA peut gÃ©nÃ©rer plans dangereux (surcharge, blessures)
- **Mitigation** :
  - RÃ¨gles sÃ©curitÃ© dans prompts (max +10% volume/semaine)
  - Validation post-gÃ©nÃ©ration (code checks)
  - Plans = suggestions, pas prescriptions mÃ©dicales

**Exemple validation** :
```python
def validate_plan_safety(plan):
    if plan.weekly_km > previous_week * 1.1:
        warnings.append("Volume increase >10%, injury risk")
```

## Fichiers Critiques Ã  CrÃ©er/Modifier

### CrÃ©er (Nouveaux)
- `/src/services/ai_coach/` : 8 modules Python (service layer)
- `/api/routers/ai_coach.py` : Router FastAPI
- `/api/models/ai_coach.py` : Pydantic models
- 5 tables BigQuery (DDL SQL)
- Bucket GCS `ela-dataplatform-ai-coach-contexts`

### Modifier (Existants)
- [pyproject.toml:19](pyproject.toml) : Ajouter `anthropic>=0.18.0`
- [api/main.py:44](api/main.py) : Register nouveau router
- `.env` : Ajouter `ANTHROPIC_API_KEY`
- GCP Secret Manager : Ajouter secret API key

### Exploiter (Existants, pas de modif)
- [src/dbt_dataplatform/models/hub/health/svc_activities.sql](src/dbt_dataplatform/models/hub/health/svc_activities.sql) : Source activitÃ©s
- [src/dbt_dataplatform/models/hub/health/svc_sleep.sql](src/dbt_dataplatform/models/hub/health/svc_sleep.sql) : Source santÃ©
- [api/routers/homepage.py](api/routers/homepage.py) : Pattern de rÃ©fÃ©rence
- [src/connectors/fetcher/gcs_writer.py](src/connectors/fetcher/gcs_writer.py) : Pattern GCS

## ObservabilitÃ©

**Logging** :
- Tous appels AI â†’ GCS `gs://ela-dataplatform-logs/ai-coach/`
- Include : prompt hash, tokens, latency, cost

**MÃ©triques** (Cloud Monitoring) :
- Latency AI requests (p50/p95/p99)
- Token usage par requÃªte
- CoÃ»ts journaliers
- Taux d'erreur

**Alertes** :
- CoÃ»t journalier > $5
- Error rate > 10%
- Latency p95 > 30s

## RÃ©sumÃ© : Leverage vs Build

### âœ… Exploiter l'Existant
- DonnÃ©es : `hub_health__svc_*` tables (28+ mÃ©triques Garmin)
- API pattern : FastAPI + Pydantic + BigQuery client
- Deployment : Cloud Run + CI/CD existants
- Infrastructure : GCS, BigQuery, IAM

### ğŸ†• Construire du Neuf
- Service AI : 8 modules Python (`/src/services/ai_coach/`)
- Tables BQ : 5 nouvelles tables
- Bucket GCS : contextes utilisateur
- API router : `/api/routers/ai_coach.py`
- SDK Anthropic : IntÃ©gration Claude Opus 4.5

### ğŸ”§ Modifier
- Dependencies : `anthropic>=0.18.0`
- Environment : `ANTHROPIC_API_KEY`
- API main : Register router

---

## RÃ©sumÃ© Estimation

**Scope MVP** : Phases 0-10

### Use Cases Couverts
âœ… **Setup Initial** (Manuel, 1x par cycle) :
- GÃ©nÃ©ration profil coureur (90j historique)
- GÃ©nÃ©ration cycle complet (4-12 semaines)
- Configuration cycle via YAML

âœ… **Automatisation Hebdomadaire** (CRON Dimanche 21h) :
- Weekly Review (.md) : Analyse complÃ¨te prÃ©vu vs rÃ©alisÃ© + santÃ©
- Weekly Plan AdaptÃ© (.md) : Plan semaine suivante basÃ© sur review + philosophie

âœ… **Infrastructure** :
- Stockage hybride (BigQuery analytics + GCS .md lisibles)
- Orchestration automatique via Cloud Scheduler
- Tests complets + template + deployment

**Estimation Temporelle** :
- Phases 0-3 (Foundation + Data + AI Profil) : ~2 semaines
- Phases 4-5 (Context + Plan Generation) : ~2 semaines
- Phase 6 (Weekly Reviewer + Orchestrator) â­ : ~2 semaines
- Phases 7-9 (API + Tests + Template) : ~1.5 semaines
- Phase 10 (Deployment + Cloud Scheduler) : ~0.5 semaine
- **Total MVP** : ~8 semaines

**CoÃ»t OpÃ©rationnel** :
- GÃ©nÃ©ration profil : $0.40 (occasionnel, 1x par cycle)
- GÃ©nÃ©ration cycle : $0.50 (occasionnel, 1x par cycle)
- **Weekly review** : $0.30/semaine Ã— 4 = **$1.20/mois** â­
- **Weekly plan** : $0.25/semaine Ã— 4 = **$1.00/mois** â­
- **Total hebdo** : ~$2.20/mois (orchestration automatique)

**ComplexitÃ©** : Moyenne-Ã©levÃ©e
- AI integration (nouveau)
- Data engineering (exploite existant)
- 10 nouveaux modules Python (dont orchestrator + reviewer)
- 6 nouvelles tables BigQuery
- 7 endpoints API (simplifiÃ©s vs 12 initialement)
- Cloud Scheduler CRON
- Cycle Config YAML (source of truth)
